{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection In Images\n",
    "\n",
    "From a high level point of view detecting something/anything in images\n",
    "require an assumption about your dataset. \n",
    "\n",
    "You can either say that all of your images contain that which you want to\n",
    "detect, or you can say that images in your dataset might contain that\n",
    "which you want to detect.\n",
    "\n",
    "If you are dealing with the first case, your problem is a regression problem in \n",
    "a continuous domain, since you are trying to locate the coordinates of your object in\n",
    "image space. Formally speaking your codomain has infinitely many values.\n",
    "\n",
    "If you are dealing with the second case, then your problem is a classification problem\n",
    "that is you try to map your input values to a limited set of labels. \n",
    "Formally speaking the size of your codomain is limited to the number of labels you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of detection in images, your input is an array of images, plus a set of features, you want to detect, and you want to obtain detected feature with the image as \n",
    "outuput.\n",
    "For example, I have a stack of images containing statues in a museum, and I want to detect\n",
    "which ones contain roman statutes and which ones contain egyptian statues.\n",
    "\n",
    "If I know that all of the images contain these statues, then my problem is a regression problem. \n",
    "If I don't know that then my problem is classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all statistical phenomena requires a measurable representation. \n",
    "\n",
    "One of the things you have to do is to transform your problem into a domain that is\n",
    "measurable and thus comparable.\n",
    "\n",
    "Almost all image detection pipelines has roughly two main steps:\n",
    "\n",
    "- Feature Description\n",
    "\n",
    "- Feature Detection/Matching\n",
    "\n",
    "You first describe what you want to detect in terms of a set of features,\n",
    "then you examine other images to see if you can match your set of features with\n",
    "another part in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can guess, there is an enormous literature in both of these domains,\n",
    "and we will be covering one feature descriptor combined with a technique for\n",
    "feature matching to illustrate the general structure of an object detection pipeline and concepts.\n",
    "\n",
    "We will start with a simple process called template matching and develop it.\n",
    "\n",
    "Let's say you have a small part of an image and you want to detect similar\n",
    "subregions in different images or in the same image, the simplest thing\n",
    "you can do is to compare sum of pixel difference between your part and the rest\n",
    "of the image.\n",
    "\n",
    "Let's implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# some utility functions\n",
    "def imshow(img: np.ndarray):\n",
    "    if len(img.shape) < 3:\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.plot()\n",
    "    \n",
    "def im2show(im1, im2):\n",
    "    f, axs = plt.subplots(1,2,figsize=(15,15))\n",
    "    fig = plt.figure()\n",
    "    if len(im1.shape) < 3:\n",
    "        axs[0].imshow(im1, cmap=\"gray\")\n",
    "        axs[1].imshow(im2, cmap=\"gray\")\n",
    "    else:\n",
    "        axs[0].imshow(im1)\n",
    "        axs[1].imshow(im2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def draw_bbox(im, bbox):\n",
    "    rect = patches.Rectangle\n",
    "    fig,ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    if len(im.shape) < 3:\n",
    "        ax.imshow(im, cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(im, figsize=(30, 25))\n",
    "    # Create a Rectangle patch\n",
    "    x,y,width, height = bbox\n",
    "    rect = patches.Rectangle((x,y),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_region_image(region: np.ndarray, image: np.ndarray):\n",
    "    \"match region in image\"\n",
    "    min_diff_val = float('inf')\n",
    "    best_positions = {}\n",
    "    region_col_nb = region.shape[1]\n",
    "    region_row_nb = region.shape[0]\n",
    "    best_positions = {}\n",
    "    for r in range(image.shape[0] - region_row_nb+1):\n",
    "        for c in range(image.shape[1] - region_col_nb+1):\n",
    "            imregion = image[r:region_row_nb+r, c:region_col_nb+c]\n",
    "            rediff = np.abs(imregion - region).sum()\n",
    "            if rediff < min_diff_val:\n",
    "                min_diff_val = rediff\n",
    "                best_positions[\"r\"] = r\n",
    "                best_positions[\"c\"] = c\n",
    "                best_positions[\"diff\"] = rediff\n",
    "    # extract from image best position\n",
    "    row, col = best_positions[\"r\"], best_positions[\"c\"]\n",
    "    imregion = image[row:region_row_nb+row, col:region_col_nb+col]\n",
    "    return imregion, best_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our function using two different but similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = ImageOps.grayscale(Image.open(\"image/im1.jpg\"))\n",
    "im2 = ImageOps.grayscale(Image.open(\"image/im2.jpg\"))\n",
    "im1arr = np.array(im1)\n",
    "im2arr = np.array(im2)\n",
    "imregion = im1arr[35:60, 55:95].copy()\n",
    "im2n = im2.resize(im1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(imregion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an image pyramid for second image.\n",
    "# a stack of same image in different resolutions\n",
    "def flexible_roi(im: Image, region: np.ndarray, fn):\n",
    "    \"flexible region of interest extraction\"\n",
    "    col, row = im.size\n",
    "    aspect_ratio = col / row # col / row\n",
    "    sizes = []\n",
    "    # for wscale in [0.6, 0.8, 1.0, 1.2, 1.4, 1.5, 1.6]:\n",
    "    for wscale in [1.0, 1.6]:\n",
    "        new_col = int(col * wscale)\n",
    "        new_row = int(new_col / aspect_ratio)\n",
    "        sizes.append((new_col, new_row))\n",
    "        \n",
    "    # creating resolution stack\n",
    "    pyramid = [np.array(im.resize(size)) for size in sizes]\n",
    "\n",
    "    minout_diff = float(\"inf\")\n",
    "    min_region = None\n",
    "    min_region_info = None\n",
    "    min_image = None\n",
    "\n",
    "    for pyramid_image in pyramid:\n",
    "        outregion, out_info = fn(region=imregion, image=pyramid_image)\n",
    "        outdiff = out_info[\"diff\"]\n",
    "        if outdiff < minout_diff:\n",
    "            min_region = outregion\n",
    "            min_region_info = out_info\n",
    "            minout_diff = outdiff\n",
    "            min_image = pyramid_image\n",
    "    return min_region_info, min_region, min_image\n",
    "\n",
    "min_region_info, min_region, min_image = flexible_roi(im2n, imregion, fn=match_region_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_region_info)\n",
    "print(min_image.shape)\n",
    "print(im2n.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(min_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2show(min_image, min_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min_region in min_image\n",
    "min_bbox = min_region_info[\"c\"], min_region_info[\"r\"], imregion.shape[1], imregion.shape[0]\n",
    "\n",
    "draw_bbox(min_image, min_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part in this algorithm was the section:\n",
    "`rediff = np.abs(imregion - region).sum()` why ?\n",
    "Because it provided us a metric for measuring how close we are to the \n",
    "finding the region. We compared regions based on their\n",
    "pixel differences.\n",
    "\n",
    "Can we compare them using a better metric ?\n",
    "Yes we absolutely can.\n",
    "And the way to do that is to represent the region in a different form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG implementation\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "def gradient_x(im):\n",
    "    return nd.sobel(im, axis=0, mode=\"constant\")\n",
    "\n",
    "def gradient_y(im):\n",
    "    return nd.sobel(im, axis=1, mode=\"constant\")\n",
    "\n",
    "def gradient_magnitude(g_x, g_y):\n",
    "    return np.hypot(g_x, g_y)\n",
    "\n",
    "def gradient_direction(g_x, g_y):\n",
    "    g_d = np.arctan(g_y / (g_x + 1e-8)) # +1e-8 to ensure non zero divison\n",
    "    g_d = np.rad2deg(g_d)\n",
    "    return g_d % 360\n",
    "\n",
    "def gradient_mag_dir(im):\n",
    "    gx, gy = gradient_x(im), gradient_y(im)\n",
    "    return gradient_magnitude(gx, gy), gradient_direction(gx, gy)\n",
    "\n",
    "def hog(img, bin_n=16):\n",
    "    \"\"\"Taken from opencv:\n",
    "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv.html?highlight=hog\n",
    "    \"\"\"\n",
    "    mag, gdir = gradient_mag_dir(img)\n",
    "    bins = np.int32(bin_n*gdir/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist\n",
    "    \n",
    "    \n",
    "def pad_with_zeros(arr1, arr2):\n",
    "    if arr1.size < arr2.size:\n",
    "        arr1 = np.pad(arr1, pad_width=(0, arr2.size - arr1.size), \n",
    "                  mode=\"constant\", constant_values=0)\n",
    "        return arr1, arr2\n",
    "    else:\n",
    "        arr2 = np.pad(arr2, pad_width=(0, arr1.size - arr2.size), \n",
    "                  mode=\"constant\", constant_values=0)\n",
    "        return arr1, arr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of computing the difference of pixels like the last time let's compute the\n",
    "difference of hog features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_region_image_hog(region: np.ndarray, image: np.ndarray):\n",
    "    \"match region in image\"\n",
    "    min_diff_val = float('inf')\n",
    "    best_positions = {}\n",
    "    region_col_nb = region.shape[1]\n",
    "    region_row_nb = region.shape[0]\n",
    "    region_hog = hog(region)\n",
    "    best_positions = {}\n",
    "    for r in range(image.shape[0] - region_row_nb+1):\n",
    "        for c in range(image.shape[1] - region_col_nb+1):\n",
    "            imregion = image[r:region_row_nb+r, c:region_col_nb+c]\n",
    "            im_hist = hog(imregion)\n",
    "            im_hist, region_hog = pad_with_zeros(im_hist, region_hog)\n",
    "            rediff = np.hypot(im_hist, region_hog)\n",
    "            renorm = np.linalg.norm(rediff)\n",
    "            if renorm < min_diff_val:\n",
    "                min_diff_val = renorm\n",
    "                best_positions[\"r\"] = r\n",
    "                best_positions[\"c\"] = c\n",
    "                best_positions[\"diff\"] = renorm\n",
    "    # extract from image best position\n",
    "    row, col = best_positions[\"r\"], best_positions[\"c\"]\n",
    "    imregion = image[row:region_row_nb+row, col:region_col_nb+col]\n",
    "    return imregion, best_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_region_info2, min_region2, min_image2 = flexible_roi(im1, imregion, \n",
    "                                                      fn=match_region_image_hog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_region_info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bbox2 = min_region_info2[\"c\"], min_region_info2[\"r\"], imregion.shape[1], imregion.shape[0]\n",
    "\n",
    "draw_bbox(min_image2, min_bbox2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of what we had seen so far has an equivalent in already established libraries.\n",
    "\n",
    "Let's see some of their interesting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template matching with opencv\n",
    "import cv2\n",
    "\n",
    "img_rgb = cv2.imread('image/im1.jpg')\n",
    "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "template = imregion.copy()\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)\n",
    "threshold = 0.8\n",
    "loc = np.where( res >= threshold)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation of background from foreground\n",
    "\n",
    "img = Image.open('image/greekstat.jpg')\n",
    "\n",
    "mask = ImageOps.grayscale(Image.open('image/greekstat.jpg'))\n",
    "mask = mask.resize(img.size)\n",
    "\n",
    "img = np.array(img)\n",
    "mask = np.array(mask)\n",
    "mask[0:500, 20:50] = 0\n",
    "mask[150:200, 600: 700] = 255\n",
    "mask[mask != 255] = 0\n",
    "Image.fromarray(mask).show()\n",
    "\n",
    "\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "rect = (50,50,450,290)\n",
    "cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]\n",
    "\n",
    "plt.imshow(img),plt.colorbar(),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation of objects\n",
    "img = cv2.imread('image/coins.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers+1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown==255] = 0\n",
    "markers = cv2.watershed(img,markers)\n",
    "img[markers == -1] = [255,0,0]\n",
    "\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And of course feature detection and pattern matching\n",
    "\n",
    "img1 = cv2.imread(\"image/im1.jpg\", 0)\n",
    "img2 = cv2.imread(\"image/im2.jpg\", 0)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "orb = cv2.ORB_create()\n",
    "#orb = cv2.FastFeatureDetector()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 10 matches.\n",
    "img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10],None, flags=2)\n",
    "\n",
    "Image.fromarray(img3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
